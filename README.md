# Robot-moving-using-Trust-region-policy-optimization-compared-to-proximal-policy-optimization

Robot moving using Trust region policy optimization compared to proximal policy optimization:
Trust Region Policy Optimization (TRPO) is an algorithm for optimizing control policies in reinforcement learning. It uses a trust region method to ensure that the update to the policy is conservative and does not make the policy worse. TRPO uses a second-order approximation of the policy's performance to determine the size of the trust region, which helps to ensure that the policy update is always improving the policy's performance.
Proximal Policy Optimization (PPO) is also an algorithm for optimizing control policies in reinforcement learning. It uses a technique called "proximal optimization" which involves optimizing a "surrogate" objective function that is similar to the true objective, but is easier to optimize. PPO uses a "clip objective" which helps to ensure that the policy update is always improving the policy's performance and also it helps to avoid large policy updates that can make the policy worse. PPO is considered more sample efficient than other algorithm like TRPO, Asynchronous Advantage Actor Critic (A3C), and others.
Trust region method:
This state of the art relay on the trust reign method and in the policy gradient families there are two famous algorithms uses trust region method to find optimization, proximal policy optimization (PPO) and trust region policy optimization (TRPO), this method help us to increase the sample efficiency and the reliability of finding the optimal policy.
