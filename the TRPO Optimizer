def unflatten_tensors(flattened, tensor_shapes):
  flattened = flattened.cpu()
  tensor_sizes = list(map(np.prod, tensor_shapes))
  indices = np.cumsum(tensor_sizes)[:-1]
  return [
      np.reshape(pair[0], pair[1]).to(device)
      for pair in zip(np.split(flattened, indices), tensor_shapes)
  ]


def _build_hessian_vector_product(func, params, reg_coeff=1e-5):
    param_shapes = [p.shape or torch.Size([1]) for p in params]
    f = func()
    f_grads = torch.autograd.grad(f, params, create_graph=True)

    def _eval(vector):
      unflatten_vector = unflatten_tensors(vector, param_shapes)
      

      assert len(f_grads) == len(unflatten_vector)
      grad_vector_product = torch.sum(
          torch.stack(
              [torch.sum(g * x) for g, x in zip(f_grads, unflatten_vector)]))

      hvp = list(
          torch.autograd.grad(grad_vector_product, params,
                              retain_graph=True))
      for i, (hx, p) in enumerate(zip(hvp, params)):
          if hx is None:
              hvp[i] = torch.zeros_like(p)

      flat_output = torch.cat([h.reshape(-1) for h in hvp])
      return flat_output + reg_coeff * vector

    return _eval


def _conjugate_gradient(f_Ax, b, cg_iters, residual_tol=1e-10):
    p = b.clone()
    r = b.clone()
    x = torch.zeros_like(b)
    rdotr = torch.dot(r, r)

    for _ in range(cg_iters):
        z = f_Ax(p)
        v = rdotr / torch.dot(p, z)
        x += v * p
        r -= v * z
        newrdotr = torch.dot(r, r)
        mu = newrdotr / rdotr
        p = r + mu * p

        rdotr = newrdotr
        if rdotr < residual_tol:
            break
    return x


class ConjugateGradientOptimizer(Optimizer):

    def __init__(self, params, max_constraint_value, cg_iters=10, max_backtracks=15,
                 backtrack_ratio=0.8, hvp_reg_coeff=1e-5, accept_violation=False):
      
        super().__init__(params, {})
        self._max_constraint_value = max_constraint_value
        self._cg_iters = cg_iters
        self._max_backtracks = max_backtracks
        self._backtrack_ratio = backtrack_ratio
        self._hvp_reg_coeff = hvp_reg_coeff
        self._accept_violation = accept_violation


    def step(self, closure):
      f_loss, f_constraint = closure()
      params = []
      grads = []
      for group in self.param_groups:
          for p in group['params']:
              if p.grad is not None:
                  params.append(p)
                  grads.append(p.grad.reshape(-1))

      flat_loss_grads = torch.cat(grads)
      f_Ax = _build_hessian_vector_product(f_constraint, params, self._hvp_reg_coeff)
      step_dir = _conjugate_gradient(f_Ax, flat_loss_grads, self._cg_iters)

      step_dir[step_dir.ne(step_dir)] = 0.

      step_size = np.sqrt(2.0 * self._max_constraint_value * (1. / (torch.dot(step_dir, f_Ax(step_dir)) + 1e-8)).cpu())

      if np.isnan(step_size):
          step_size = 1.

      descent_step = step_size * step_dir
      self._backtracking_line_search(params, descent_step, f_loss, f_constraint)


    def _backtracking_line_search(self, params, descent_step, f_loss, f_constraint):
        prev_params = [p.clone() for p in params]
        ratio_list = self._backtrack_ratio**np.arange(self._max_backtracks)
        loss_before = f_loss()

        param_shapes = [p.shape or torch.Size([1]) for p in params]
        descent_step = unflatten_tensors(descent_step, param_shapes)
        assert len(descent_step) == len(params)

        for ratio in ratio_list:
            for step, prev_param, param in zip(descent_step, prev_params, params):
                step = ratio * step
                new_param = prev_param.data - step
                param.data = new_param.data

            loss = f_loss()
            constraint_val = f_constraint()
            if (loss < loss_before and constraint_val <= self._max_constraint_value):
                break
